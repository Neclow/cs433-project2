{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run.ipynb","provenance":[{"file_id":"1IaXHtyw5Fs7rMcHBasulwv0kdaLGJ5em","timestamp":1576689798505},{"file_id":"1XpI8ZlrxO1AbKvbbx0VZmwyBEEikTUEl","timestamp":1575197351421},{"file_id":"1cuE44qsz1d-q_bs4QPeL4s2O-ZFDutOs","timestamp":1575152091503}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"E7xp9bxojA90","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","# A python library for data observation and data analysis\n","import pandas as pd\n","\n","# A Python library for Machine Learning\n","import sklearn\n","from sklearn import metrics # All kinds of metrics\n","from sklearn.model_selection import train_test_split # For splitting data in train and test sets\n","from sklearn.utils import shuffle, class_weight # Shuffling of data, calculator of class weights\n","from sklearn.model_selection import cross_val_score, GridSearchCV # Cross-validation score, grid search with cross-validation\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler # Standardization, normalization\n","\n","# A Python library to deal with imbalanced data\n","import imblearn\n","from imblearn.over_sampling import SMOTE # SMOTE algorithm for oversampling\n","from imblearn.under_sampling import RandomUnderSampler # Random undersampling\n","from imblearn.pipeline import Pipeline # To combine undersampling/overampling with grid search\n","\n","# For counting elements in a dictionary\n","import collections\n","from collections import Counter"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X_YzwABB7-BV","colab_type":"text"},"source":["Core model zoo"]},{"cell_type":"code","metadata":{"id":"yY3EQ13L7Vl2","colab_type":"code","colab":{}},"source":["'''\n","    Core model zoo:\n","    - XGBoost: optimized and scalable gradient tree boosting library\n","    - LightGBM: gradient boosting framework using decision trees\n","    - Multi-layer perceptron: a simple design of an artificial neuron network (ANN)\n","    - K-nearest neighbors: classification through finding of k nearest neighbors of each input\n","'''\n","\n","# XGBoost Classifier\n","import xgboost as xgb \n","# LightGBM Classifier\n","from lightgbm import LGBMClassifier \n","# Multi-layer Perceptron\n","from sklearn.neural_network import MLPClassifier \n","# K-Nearest Neighbors\n","from sklearn.neighbors import KNeighborsClassifier"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E_WquDd7jN9R","colab_type":"code","outputId":"32ab48a5-4852-44c0-f750-6fd8fcc3df6a","executionInfo":{"status":"ok","timestamp":1576765429562,"user_tz":-60,"elapsed":1009,"user":{"displayName":"Gru esome","photoUrl":"","userId":"14245716998858390986"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Mount Colab with local drive to access data\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2w90TUO18FfV","colab_type":"text"},"source":["Load train data and labels for each timepoint"]},{"cell_type":"code","metadata":{"id":"WeTC-A0Vv8jP","colab_type":"code","colab":{}},"source":["def load_data(n_groups=4):\n","  '''\n","    Load training for all four timepoints (only the free latter were used)\n","\n","    Returns: training data, labels, unlabeled data and metadata relative to unlabeled data for each timepoint\n","  '''\n","\n","  x_groups = [] # training data\n","  y_groups = [] # training labels\n","  t_groups = [] # unlabeled data\n","  m_groups = [] # metadata relative to unlabeled data (i.e., well and plate number) --> used for predicting unlabeled data\n","\n","  for i in range(n_groups):\n","    train_fname = 'gdrive/My Drive/ML2/data/labeled_' + repr(i) + '.csv'\n","\n","    # Collect training data for each timepoint as a NumPy array\n","    x_groups.append(np.asarray(pd.read_csv(train_fname))) \n","\n","    label_fname = 'gdrive/My Drive/ML2/data/labels_' + repr(i) + '.csv'\n","    label_group = pd.read_csv(label_fname)\n","\n","    # Collect labels for each timepoint as a NumPy array\n","    y_groups.append(np.reshape(np.asarray(label_group), (len(label_group),)))\n","\n","    # Check that train data and labels have same dimensions\n","    assert(x_groups[i].shape[0] == y_groups[i].shape[0])\n","    print('OK. Train data and labels at timepoint ' + repr(i) + ' have same x-dimensions (' + repr(x_groups[i].shape[0]) + ').')\n","\n","    # Collect unlabeled data for each timepoint as a NumPy array\n","    test_fname = 'gdrive/My Drive/ML2/data/unlabeled_' + repr(i) + '.csv'\n","    t_groups.append(np.asarray(pd.read_csv(test_fname)))\n","\n","    print('Unlabeled data at timepoint ' + repr(i) + ': ' + repr(t_groups[i].shape[0]))\n","\n","    '''# Collect metadata relative to unlabeled data for each timepoint as a NumPy array\n","    meta_fname = 'gdrive/My Drive/ML2/data/meta_unlabeled_' + repr(i) + '.csv'\n","    m_groups.append(np.asarray(pd.read_csv(meta_fname)))'''\n","  \n","  return x_groups, y_groups, t_groups"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hAc42pGtt9ld","colab_type":"code","outputId":"b94496d8-edc6-4133-feba-0ebbcbb5ad4f","executionInfo":{"status":"ok","timestamp":1576765467651,"user_tz":-60,"elapsed":32907,"user":{"displayName":"Gru esome","photoUrl":"","userId":"14245716998858390986"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["x_groups, y_groups, t_groups = load_data()"],"execution_count":27,"outputs":[{"output_type":"stream","text":["OK. Train data and labels at timepoint 0 have same x-dimensions (41392).\n","Unlabeled data at timepoint 0: 288156\n","OK. Train data and labels at timepoint 1 have same x-dimensions (45138).\n","Unlabeled data at timepoint 1: 308990\n","OK. Train data and labels at timepoint 2 have same x-dimensions (47781).\n","Unlabeled data at timepoint 2: 321565\n","OK. Train data and labels at timepoint 3 have same x-dimensions (51846).\n","Unlabeled data at timepoint 3: 340905\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ailUXStbblUF"},"source":["Grid search (not used until now)"]},{"cell_type":"code","metadata":{"id":"BPJiA7jL0631","colab_type":"code","colab":{}},"source":["def grid_search(i, n_folds=5, method='lgbm', smote=False, rus=False):\n","  '''\n","    Grid search combined with 5-fold cross-validation\n","    i: timepoint number\n","    n_folds: number of folds for cross-validation\n","    method: ML algorithm to test (LGBM, XGBoost, MLP, kNN)\n","    smote: Perform grid search with oversampling using SMOTE\n","    rus: Perform grid search with undersampling using random undersampling\n","\n","    return: optimal hyperparameters, merged with default parameters\n","  '''\n","\n","  # Split labeled data in train and test set\n","  print('Splitting data...')\n","  X_train, X_test, Y_train, Y_test = train_test_split(x_groups[i], y_groups[i], test_size=0.1, random_state=42)\n","  class_weights = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train) # Get class weights\n","\n","  # Dimension of Train and Test set \n","  print(\"Dimension of Train set:\", X_train.shape)\n","  print(\"Dimension of Test set:\", X_test.shape)\n","\n","  # Standardization of labeled data\n","  print('Preprocessing...')\n","  scaler = StandardScaler()\n","  X_train_scaled = scaler.fit_transform(X_train)\n","  X_test_scaled = scaler.transform(X_test)\n","\n","  if method == 'lgbm':\n","    ''' \n","      Parameter grid for LGBM:\n","      num_leaves: maximum of number of leaves in a tree\n","      n_estimators: number of boosting iterations\n","      reg_alpha: L1-regularization term\n","      reg_beta: L2-regularization term\n","    '''\n","\n","    '''\n","      Default parameters for LGBM:\n","      boosting_type: Gradient boosting with decision trees (GBDT)\n","      objective: multiclass classification\n","      class_weight: balanced\n","      random_state: to preserve same results for each run\n","    '''\n","\n","    default_param = {'boosting_type':'gbdt', 'objective':'multiclass', \n","                       'class_weight':'balanced', 'random_state': 42} \n","\n","    if smote:\n","      # Create pipeline if the user wants to test SMOTE with grid search\n","      print('Grid search will be performed with SMOTE.')\n","      \n","      model = Pipeline([\n","        ('sampling', SMOTE()),\n","        ('classification', LGBMClassifier())\n","      ])\n","      params_grid = {'classification__num_leaves':[10, 31, 50], 'classification__n_estimators':[50, 100, 200], \n","                 'classification__reg_alpha':[0, 0.1], 'classification__reg_beta':[0, 0.1]}\n","    \n","    elif rus:\n","      # Create pipeline if the user wants to test random undersampling with grid search\n","      print('Grid search will be performed with random undersampling.')\n","\n","      model = Pipeline([\n","        ('sampling', RandomUnderSampler(random_state=42)),\n","        ('classification', LGBMClassifier(**default_param))\n","      ])\n","      params_grid = {'classification__num_leaves':[10, 31, 50], 'classification__n_estimators':[50, 100, 200], \n","                 'classification__reg_alpha':[0, 0.1], 'classification__reg_beta':[0, 0.1]}\n","\n","    else:\n","      model = LGBMClassifier(**default_param)\n","    \n","      params_grid = {'num_leaves': [10, 31, 50], 'n_estimators':[50, 100, 200], \n","                     'reg_alpha':[0, 0.1], 'reg_beta':[0, 0.1]}\n","  \n","  elif method == 'xgb':\n","    ''' \n","      Parameter grid for XGBoost:\n","      max_depth: maximum tree depth\n","      min_child_weight: minimum sum of instance weight needed in a child (leaf).\n","      subsample: subsample ratio of the training instances\n","      lambda: L2-regularization term\n","    '''\n","\n","    '''\n","      Default parameters for XGBoost:\n","      objective: multiclass classification with softmax\n","      tree_method, gpu_id: methods to use XGBoost using Colab's GPU\n","      sample_weights: class weights (to account for imbalanced dataset)\n","    '''\n","\n","    default_param = {'objective': 'multi:softmax', 'tree_method': 'gpu_hist', \n","                     'gpu_id': '0', 'sample_weights': class_weights}\n","\n","    model = xgb.XGBClassifier(**default_param)\n","\n","    params_grid = {'max_depth':[3, 6, 9], 'min_child_weight':[0.25, 0.5, 1], \n","                   'subsample':[0.5, 1], 'lambda':[0.1, 1, 10]}\n","  \n","  elif method == 'mlp':\n","    ''' \n","      Parameter grid for MLP:\n","      hidden_layer_sizes: size of hidden layers\n","      solver: solver for weight optimization (Adam or LBFGS)\n","      alpha: L2-regularization term\n","      activation: activation function or each layer (ReLU or tanh)\n","    '''\n","\n","    '''\n","      Default parameters for MLP:\n","      max_iter: maximum number of iterations, to let the method converge\n","    '''\n","    default_param = {'max_iter':1000}\n","\n","    model = MLPClassifier(**default_param)\n","\n","    params_grid={'hidden_layer_sizes':[50, 100, 200], 'solver':['adam', 'lbfgs'], 'alpha':[0.01, 0.001, 0.0001], 'activation':['relu', 'tanh']}\n","  \n","  elif method == 'knn':\n","    ''' \n","      Parameter grid for kNN:\n","      n_neigbors: number of neighbors of given input to consider\n","      weights: weight function used in prediction\n","      metric: distance metric used to build the tree\n","    '''\n","    default_param = {'algorithm': 'auto'}\n","    model = KNeighborsClassifier(**default_param)\n","\n","    params_grid={'n_neighbors':[3, 5, 10, 20, 50, 100], 'weights':['distance', 'uniform'], 'metric':['euclidean', 'manhattan']}\n","  else:\n","    raise NameError('Unknown method.')\n","\n","  print('Training...')\n","  # Grid search with 5-fold cross-validation\n","  grid_search_model = GridSearchCV(model, params_grid, cv=n_folds, verbose=5, n_jobs=-1)\n","  grid_search_model.fit(X_train_scaled, Y_train)\n","\n","  # Print best cross-validation score found using grid search\n","  print('Best score:', grid_search_model.best_score_,\"\\n\") \n","\n","  # View optimal hyperparameters found using grid search\n","  print('Best params:', grid_search_model.best_params_)\n","\n","  return {**default_param, **grid_search_model.best_params_}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rKktBX1h37KT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2d1f828a-bdff-48fe-cf34-a63e4c537537","executionInfo":{"status":"ok","timestamp":1576765467662,"user_tz":-60,"elapsed":32320,"user":{"displayName":"Gru esome","photoUrl":"","userId":"14245716998858390986"}}},"source":["# Uncomment to perform grid search with 5-fold cross-validation (duration: 60-120 min):\n","'''\n","best_params = []\n","n_groups = 4\n","for i in range(1, n_groups):\n","  best_params.append(grid_search(i, method='lgbm'))\n","'''"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nbest_params = []\\nn_groups = 4\\nfor i in range(1, n_groups):\\n  best_params.append(grid_search(i, method='lgbm'))\\n\""]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"MqrWYcy8-rbZ","colab_type":"text"},"source":["Grid search solutions"]},{"cell_type":"code","metadata":{"id":"xbkrrKIb2gfH","colab_type":"code","colab":{}},"source":["# GridSearhCV solutions for each method and each timepoint\n","\n","##### LGBM Grid search results (no resampling, oversampling with SMOTE, undersampling with RUS)\n","default_lgbm_param = {'boosting_type':'gbdt', 'objective':'multiclass', \n","                       'class_weight':'balanced', 'random_state': 42} \n","\n","best_lgbm_params = [{**default_lgbm_param, 'n_estimators':200, 'num_leaves':50, 'reg_alpha':0.1, 'reg_beta':0}, \n","                    {**default_lgbm_param, 'n_estimators':200, 'num_leaves':50, 'reg_alpha':0, 'reg_beta':0}, \n","                    {**default_lgbm_param, 'n_estimators':200, 'num_leaves':50, 'reg_alpha':0, 'reg_beta':0}, \n","                   ]\n","\n","best_smote_param = [{**default_lgbm_param, 'n_estimators': 200, 'num_leaves': 50, 'reg_alpha': 0, 'reg_beta': 0},\n","                    {**default_lgbm_param, 'n_estimators': 200, 'num_leaves': 50, 'reg_alpha': 0, 'reg_beta': 0},\n","                    {**default_lgbm_param, 'n_estimators': 200, 'num_leaves': 50, 'reg_alpha': 0, 'reg_beta': 0}\n","                   ]\n","\n","best_rus_params = [{**default_lgbm_param, 'n_estimators': 50, 'num_leaves': 10, 'reg_alpha': 0, 'reg_beta': 0},\n","                   {**default_lgbm_param, 'n_estimators': 50, 'num_leaves': 10, 'reg_alpha': 0, 'reg_beta': 0},\n","                   {**default_lgbm_param, 'n_estimators': 50, 'num_leaves': 10, 'reg_alpha': 0, 'reg_beta': 0}\n","                  ]\n","\n","##### XGB Grid search results\n","default_xgb_param = {'objective': 'multi:softmax', 'tree_method': 'gpu_hist', 'gpu_id': '0'}\n","\n","best_xgb_params = [{**default_xgb_param, 'max_depth': 9, 'min_child_weight': 0.5, 'subsample': 1, 'lambda':0.1}, \n","                    {**default_xgb_param, 'max_depth': 9, 'min_child_weight': 0.25, 'subsample': 1, 'lambda':0.1}, \n","                    {**default_xgb_param, 'max_depth': 9, 'min_child_weight': 0.25, 'subsample': 1, 'lambda':0.1}, \n","                   ]\n","\n","##### kNN Grid search results\n","default_knn_param = {'algorithm':'auto'}\n","\n","\n","best_knn_params = [{**default_knn_param, 'n_neighbors': 5, 'distance': 'manhattan', 'weights':'distance'}, \n","                   {**default_knn_param, 'n_neighbors': 5, 'distance': 'manhattan', 'weights':'distance'}, \n","                   {**default_knn_param, 'n_neighbors': 3, 'distance': 'manhattan', 'weights':'distance'}, \n","                  ]\n","\n","##### MLP Grid search results\n","default_mlp_param = {'max_iter':1000}\n","\n","best_mlp_params = [{**default_mlp_param, 'hidden_layer_sizes':200, 'solver': 'adam', 'activation': 'relu', 'alpha':0.001}, \n","                   {**default_mlp_param, 'hidden_layer_sizes':200,'solver': 'adam', 'activation': 'relu', 'alpha':0.01}, \n","                   {**default_mlp_param, 'hidden_layer_sizes':200, 'solver': 'adam', 'activation': 'relu', 'alpha':0.001}, \n","                  ]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mT0IQfBU-1iB","colab_type":"text"},"source":["Training method"]},{"cell_type":"code","metadata":{"id":"b2OAHI6RaMyK","colab_type":"code","colab":{}},"source":["def train(i, method, model_params, smote=False, rus=False):\n","  '''\n","    i: timepoint number [0, 1 or 2]\n","    smote: boolean to use SMOTE (if True)\n","    rus: boolean to use random undersampling (if True)\n","\n","    returns: model used, tested parameters and predictions on unlabeled dataset\n","  '''\n","\n","  print('---------- Timepoint ' + repr(i+1) + ' ----------')\n","  \n","  # Split train and test set using 85% of labeled data\n","  print('Splitting data...')\n","  X_train, X_test, Y_train, Y_test = train_test_split(x_groups[i+1], y_groups[i+1], test_size=0.15, random_state=42)\n","  X_unlabeled = t_groups[i+1]\n","\n","  # Dimension of Train and Test set \n","  print(\"Dimension of Train set\", X_train.shape)\n","  print(\"Dimension of Test set\", X_test.shape,\"\\n\")\n","\n","  # Standardization\n","  print('Preprocessing...')\n","  scaler = StandardScaler()\n","  X_train_scaled = scaler.fit_transform(X_train)\n","  X_test_scaled = scaler.transform(X_test)\n","  X_unlabeled_scaled = scaler.transform(X_unlabeled)\n","\n","  # Compute class weights (used in some of our ML methods)\n","  class_weights = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)\n","\n","  if smote:\n","    # Oversampling with SMOTE\n","    smote_ = SMOTE()\n","    X_train_scaled, Y_train = smote_fit.sample(X_train_scaled, Y_train)\n","  elif rus:\n","    # Undersampling with random undersampling\n","    rus_ = RandomUnderSampler(random_state=42)\n","    X_train_scaled, Y_train = rus_.fit_resample(X_train_scaled, Y_train)\n","  \n","  param_lgbm = {'boosting_type': 'gbdt','objective': 'multiclass', 'random_state': 42, 'n_estimators': 200, \n","                'num_leaves': 50, 'reg_alpha': 0.1, 'reg_beta': 0}\n","\n","  print('Training...')\n","  # Build model with inputed parameters\n","  if method == 'lgbm':\n","    model = LGBMClassifier(**param_lgbm)\n","  elif method == 'xgb':\n","    model_params.update({'sample_weights': class_weights})\n","    model = xgb.XGBClassifier(**model_params)\n","  elif method == 'mlp':\n","    model = MLPClassifier(**model_params)\n","  elif method == 'knn':\n","    model = KNeighborsClassifier(**model_params)\n","  else:\n","    raise NameError('Unknown method.')\n","\n","  # Print model for user \n","  print(model)\n","\n","  # Train model\n","  model.fit(X_train_scaled, Y_train)\n","\n","  print('Predicting...')\n","  # Predict test sest\n","  Y_pred = model.predict(X_test_scaled)\n","\n","  # Print confusion matrix\n","  print('Confusion matrix')\n","  print(metrics.confusion_matrix(Y_test,Y_pred))\n","  print(\"\\n\") \n","\n","  # Print evaluation metrics\n","  print('MCC score:', sklearn.metrics.matthews_corrcoef(Y_test, Y_pred))\n","  print(\"Training set score: %f\" % model.score(X_train_scaled, Y_train))\n","  print(\"Testing  set score: %f\" % model.score(X_test_scaled, Y_test ))\n","\n","  # Give predictions for unlabeled dataset\n","  print('Predicting unlabeled test set...')\n","  pred = model.predict(X_unlabeled_scaled)\n","  \n","  return Counter(pred)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5HgEJ5oZ_VbZ","colab_type":"text"},"source":["Train for each timepoint"]},{"cell_type":"code","metadata":{"id":"JN4_3jWJY6-4","colab_type":"code","outputId":"663380fa-e16f-4175-cc72-dd98f0418d8a","executionInfo":{"status":"ok","timestamp":1576765678090,"user_tz":-60,"elapsed":203232,"user":{"displayName":"Gru esome","photoUrl":"","userId":"14245716998858390986"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["n_groups = 4\n","for i in range(n_groups-1):\n","  unlabeled_pred = train(i, 'lgbm', best_lgbm_params[i])\n","  # Print unlabeled predictions as a Counter \n","  # A counter keeps track of how many times equivalent values are found in an array\n","  print(unlabeled_pred)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["---------- Timepoint 1 ----------\n","Splitting data...\n","Dimension of Train set (38367, 94)\n","Dimension of Test set (6771, 94) \n","\n","Preprocessing...\n","Training...\n","LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n","               importance_type='split', learning_rate=0.1, max_depth=-1,\n","               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n","               n_estimators=200, n_jobs=-1, num_leaves=50,\n","               objective='multiclass', random_state=42, reg_alpha=0.1,\n","               reg_beta=0, reg_lambda=0.0, silent=True, subsample=1.0,\n","               subsample_for_bin=200000, subsample_freq=0)\n","Predicting...\n","Confusion matrix\n","[[1096    2   29    4   25    6    0    3   52   52]\n"," [  28  203    2    6   11    9    0    0    9    4]\n"," [  89    4  536    1   12    0    0    3   75   80]\n"," [  42    5    1  159    1    4    0    0    9    4]\n"," [  43    2   20    2  988    0    3    1   37   66]\n"," [  29    2    3    0    4  254    2    0    8   12]\n"," [   2    1    2    1   11    1   95    4    3    3]\n"," [  24    1    0    0    9    0    3  121    1    0]\n"," [  99    1   42    2   69    8    0    0  843  128]\n"," [  83    0   40    0   52   11    1    0   91  977]]\n","\n","\n","MCC score: 0.7390768927280226\n","Training set score: 0.965100\n","Testing  set score: 0.778615\n","Predicting unlabeled test set...\n","Counter({0.0: 103920, 4.0: 64987, 8.0: 52561, 9.0: 49474, 2.0: 12774, 5.0: 9674, 3.0: 7138, 1.0: 6227, 7.0: 1566, 6.0: 669})\n","---------- Timepoint 2 ----------\n","Splitting data...\n","Dimension of Train set (40613, 94)\n","Dimension of Test set (7168, 94) \n","\n","Preprocessing...\n","Training...\n","LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n","               importance_type='split', learning_rate=0.1, max_depth=-1,\n","               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n","               n_estimators=200, n_jobs=-1, num_leaves=50,\n","               objective='multiclass', random_state=42, reg_alpha=0.1,\n","               reg_beta=0, reg_lambda=0.0, silent=True, subsample=1.0,\n","               subsample_for_bin=200000, subsample_freq=0)\n","Predicting...\n","Confusion matrix\n","[[1066    0   47    0   39    0    0    0   59   44]\n"," [  18  215    3    0   15    7    0    0    1    3]\n"," [ 101    0  683    2   18    1    0    3   61   57]\n"," [   2    1   13  219    0    8    0    0   12    3]\n"," [  57    4   19    0 1009    6    1    6   50   40]\n"," [  25    5   11    2    3  339    0    0    7   12]\n"," [   4    0    1    0    8    0  133    3    4    2]\n"," [  18    0    2    0    6    0    3  159    4    2]\n"," [ 102    0   37    4   75    0    1    0  984  100]\n"," [  92    1   56    1   45    5    2    0   98  919]]\n","\n","\n","MCC score: 0.7647165069473454\n","Training set score: 0.961047\n","Testing  set score: 0.798828\n","Predicting unlabeled test set...\n","Counter({0.0: 78856, 9.0: 60978, 8.0: 60414, 4.0: 57261, 2.0: 25799, 5.0: 19527, 1.0: 8162, 3.0: 5169, 7.0: 3728, 6.0: 1671})\n","---------- Timepoint 3 ----------\n","Splitting data...\n","Dimension of Train set (44069, 94)\n","Dimension of Test set (7777, 94) \n","\n","Preprocessing...\n","Training...\n","LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n","               importance_type='split', learning_rate=0.1, max_depth=-1,\n","               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n","               n_estimators=200, n_jobs=-1, num_leaves=50,\n","               objective='multiclass', random_state=42, reg_alpha=0.1,\n","               reg_beta=0, reg_lambda=0.0, silent=True, subsample=1.0,\n","               subsample_for_bin=200000, subsample_freq=0)\n","Predicting...\n","Confusion matrix\n","[[1067    0   51    0   38    0    0    1   62   64]\n"," [   8  270    9    5    5    2    0    0    6    9]\n"," [  58    1 1250    0   23    5    0    4   41   56]\n"," [   3    2    6  218    3    1    0    0    7    2]\n"," [  54    5   60    0  901    1    7    3   40   54]\n"," [   8    4   14    1    1  297    0    0   12    8]\n"," [   2    0    9    0    4    0  118    3    3    1]\n"," [   8    0    8    0    2    0    0  176    9    5]\n"," [  52    0   34    1   39    0    0    0 1190  118]\n"," [  37    3   47    0   36    4    0    0   84 1037]]\n","\n","\n","MCC score: 0.8106443339421345\n","Training set score: 0.966711\n","Testing  set score: 0.838884\n","Predicting unlabeled test set...\n","Counter({8.0: 88551, 2.0: 74669, 9.0: 62903, 0.0: 48814, 4.0: 32285, 5.0: 17998, 1.0: 7292, 7.0: 3662, 6.0: 2554, 3.0: 2177})\n"],"name":"stdout"}]}]}